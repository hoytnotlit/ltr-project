{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "modified-petite",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import timeit\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "straight-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Processing SwELL data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "serial-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter data to get essays containing S-Msubj errors\n",
    "\n",
    "data_src = \"./SweLL_release_v1/SweLL_Gold/SweLL_Gold/swellData.json\"\n",
    "correction_data_src = \"SweLL_correctionTags_502essays.json\"\n",
    "\n",
    "result = {}\n",
    "with open(data_src, 'r') as data, open(correction_data_src, 'r') as data2:\n",
    "    json_data = json.load(data)\n",
    "    json_data2 = json.load(data2)\n",
    "    for essay_id in json_data:\n",
    "        corrections = json.loads(json_data2[essay_id])\n",
    "        # get edges where label 'S-Msubj' exists\n",
    "        essay_corrs = []\n",
    "        for corr in corrections[\"edges\"].values():\n",
    "            if \"S-Msubj\" in corr[\"labels\"]:\n",
    "                essay_corrs.append(corr)\n",
    "        if len(essay_corrs) > 0:\n",
    "            result[essay_id] = {'source':corrections[\"source\"], 'target':corrections[\"target\"], 'edges':essay_corrs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "graduate-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions for constructing sentences \n",
    "\n",
    "end_tokens = \".‚ê§\"\n",
    "\n",
    "def get_target_sentence(essay_data, edge_id):\n",
    "    target_sentence = []\n",
    "    for item in essay_data['target']:\n",
    "        if item['id'] == edge_id:\n",
    "            token = item\n",
    "            break\n",
    "\n",
    "    token_id = essay_data['target'].index(token)\n",
    "    next_token = essay_data['target'][token_id]\n",
    "\n",
    "    while end_tokens.find(next_token['text'].strip()) < 0 and token_id < len(essay_data['target']): \n",
    "        next_token = essay_data['target'][token_id]\n",
    "        target_sentence.append(next_token['text'])\n",
    "        token_id += 1\n",
    "\n",
    "    prev_token_id = essay_data['target'].index(token) - 1\n",
    "    prev_token = essay_data['target'][prev_token_id]\n",
    "    prev = []\n",
    "    while end_tokens.find(prev_token['text'].strip()) < 0: \n",
    "        prev.append(prev_token['text'])\n",
    "        prev_token_id -= 1\n",
    "        prev_token = essay_data['target'][prev_token_id]\n",
    "    prev.reverse()\n",
    "\n",
    "    token_index = len(prev)\n",
    "    target_sentence = prev + target_sentence\n",
    "\n",
    "    return \"\".join(target_sentence), target_sentence[token_index], (prev_token_id+1)\n",
    "\n",
    "def get_source_sentence(essay_data, edge_id, start_token_id):\n",
    "    source_sentence = []\n",
    "    start_token_id = f's{start_token_id}'\n",
    "\n",
    "    start_token = None\n",
    "    for item in essay_data['source']:\n",
    "        if item['id'] == start_token_id:\n",
    "            start_token = item\n",
    "            break\n",
    "    \n",
    "    if start_token == None:\n",
    "        return\n",
    "    \n",
    "    token_id = essay_data['source'].index(start_token)\n",
    "    next_token = essay_data['source'][token_id]\n",
    "\n",
    "    while end_tokens.find(next_token['text'].strip()) < 0 and token_id < len(essay_data['source']): \n",
    "        next_token = essay_data['source'][token_id]\n",
    "        source_sentence.append(next_token['text'])\n",
    "        token_id += 1\n",
    "        \n",
    "    prev_token_id = essay_data['source'].index(start_token) - 1\n",
    "    prev_token = essay_data['source'][prev_token_id]\n",
    "    prev = []\n",
    "\n",
    "    while end_tokens.find(prev_token['text'].strip()) < 0 and prev_token_id > -1:\n",
    "        prev.append(prev_token['text'])\n",
    "        prev_token_id -= 1\n",
    "        prev_token = essay_data['source'][prev_token_id]\n",
    "    prev.reverse()\n",
    "\n",
    "    return \"\".join(prev + source_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "confused-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose target-source pairs of sentences\n",
    "# NOTE: this is not completely accurate, sometimes the source sentence does not corresponde to the actual source\n",
    "\n",
    "missing_tokens = []\n",
    "target_source_pairs = []\n",
    "\n",
    "for key in result:\n",
    "    for edge in result[key]['edges']:\n",
    "        target_sentence, missing_token, start_token_id = get_target_sentence(result[key], edge['id'].replace(\"e-\", \"\"))\n",
    "        missing_tokens.append(missing_token)\n",
    "        source_sentence = get_source_sentence(result[key], edge['id'].replace(\"e-\", \"\"), start_token_id)\n",
    "        target_source_pairs.append((target_sentence, source_sentence, missing_token.strip(), key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-viking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze how/when/why S-Msubj errors occur/subject is missing\n",
    "\n",
    "# list tokens that were missing\n",
    "token_counts = Counter(missing_tokens).most_common()\n",
    "token_df = pd.DataFrame(token_counts, columns=['token', 'count'])\n",
    "token_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "durable-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for printing a table of target-source pairs, \n",
    "# optional parameter to view sentences based on given token\n",
    "\n",
    "def show_sent_pairs(missing_token=None):\n",
    "    pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None) \n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "    d = {'target': [], 'source': [], 'key': []}\n",
    "\n",
    "    for tsp in target_source_pairs:\n",
    "        if missing_token == None or tsp[2] == missing_token:\n",
    "            d['target'].append(tsp[0])\n",
    "            d['source'].append(tsp[1])\n",
    "            d['key'].append(tsp[3])\n",
    "\n",
    "    df = pd.DataFrame(data=d)\n",
    "    df.style.set_properties(**{'text-align': 'left'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sent_pairs('jag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pediatric-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Sparv-tagging\n",
    "# modified from the given script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "clinical-number",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# URL to the Sparv 2 API:\n",
    "sparv_url = \"https://ws.spraakbanken.gu.se/ws/sparv/v2/\"\n",
    "\n",
    "# Optional settings, specifying what analysis should be done (in this case compound analysis only):\n",
    "# Check https://ws.spraakbanken.gu.se/ws/sparv/v2/#settings for more info\n",
    "sparv_settings = json.dumps({\n",
    "    \"corpus\": \"Korpusnamn\",\n",
    "    \"lang\": \"sv\",\n",
    "    \"textmode\": \"plain\",\n",
    "    \"positional_attributes\": {\n",
    "        \"dependency_attributes\":[\"ref\",\"dephead\",\"deprel\"],\n",
    "        \"lexical_attributes\": [\n",
    "            \"pos\",\n",
    "            \"msd\",\n",
    "            \"lemma\"\n",
    "        ]\n",
    "    },\n",
    "    \"text_attributes\": {\n",
    "        \"readability_metrics\": [\n",
    "            \"lix\",\n",
    "            \"ovix\",\n",
    "            \"nk\"\n",
    "        ]\n",
    "    }\n",
    "})\n",
    "\n",
    "query_parameters = {\"text\": None, \"settings\": sparv_settings}\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "# Example using Python's built-in urllib.request\n",
    "# https://docs.python.org/3/library/urllib.request.html\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "def sparv_urllib_req(text):\n",
    "    query_parameters = {\"text\": text, \"settings\": sparv_settings}\n",
    "    enc_query = urllib.parse.urlencode(query_parameters).encode(\"UTF-8\")\n",
    "    resp = urllib.request.urlopen(sparv_url, data=enc_query).read()\n",
    "    return resp.decode(\"UTF-8\")\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "# Example using the more intuitive but third-pary Requests library\n",
    "# https://docs.python-requests.org/en/master/\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "import requests\n",
    "\n",
    "def sparv_req(text):\n",
    "    query_parameters = {\"text\": text, \"settings\": sparv_settings}\n",
    "    response = requests.get(sparv_url, params=query_parameters)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "brutal-sherman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for parsing xml response\n",
    "\n",
    "def get_sentence_pos(tagged_sentence):\n",
    "    root = ET.fromstring(tagged_sentence)\n",
    "    word_tags = root.findall(\"./corpus/text/paragraph/sentence/\")\n",
    "    #for wt in word_tags:\n",
    "    #    print(wt.attrib, wt.text) \n",
    "    return [{'deprel': wt.attrib['deprel'], 'msd': wt.attrib['msd']} for wt in word_tags] #wt.attrib['msd'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "combined-outreach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Corrupt data - final scrip in corrupt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-outreach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data for corruption\n",
    "tree = ET.parse('leo_coctaill.xml')\n",
    "root = tree.getroot()\n",
    "clean_data = [\" \".join([word.text for word in sent]) for sent in root]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_orig_sent(modified_sent):\n",
    "    for i, sent in enumerate(clean_sentences):\n",
    "        if modified_sent.lower() in sent.lower():\n",
    "            print(i, sent)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_tags = ['SS', 'SP', 'ES', 'FS'] # labels from https://cl.lingfil.uu.se/~nivre/swedish_treebank/dep.html\n",
    "corrupt_sentences = [] # NOTE: not all sentences will necessarily be corrupt \n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "start_i = 0\n",
    "\n",
    "if os.path.isfile('corrupted_extra.txt'):\n",
    "    with open('corrupted_extra.txt', 'r') as f:\n",
    "        file_lines = f.readlines()\n",
    "        if len(file_lines) > 0:\n",
    "            start_i = int(file_lines[-1].split(\"\\t\")[0]) + 1\n",
    "            \n",
    "    with open('corrupted_extra.txt', 'a+') as corrupted_data:\n",
    "        for i, sentence in enumerate(clean_data[start_i:], start_i):\n",
    "            elapsed = timeit.default_timer() - start_time\n",
    "            \n",
    "            print(f\"\\rprocessing sentence {i} / {len(clean_data)} time elapsed: {elapsed}\", end=\"\")\n",
    "            \n",
    "            try:\n",
    "                tagged = sparv_req(sentence)\n",
    "                prevelapsed = elapsed\n",
    "            except:\n",
    "                print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "                time.sleep(60)\n",
    "                continue\n",
    "            tagged_dict = get_sentence_pos(tagged)\n",
    "            \n",
    "            # NOTE: only one corruption is applied per sentence\n",
    "\n",
    "            # find subjects in sentence\n",
    "            sent_subjs = [i for i, item in enumerate(tagged_dict) if 'SUB' in item['msd']]\n",
    "            sentence_split = sentence.split()\n",
    "\n",
    "            # case: subject has already been mentioned\n",
    "            # NOTE: not sure if this is the best way to check if subjects are same\n",
    "            if len(sent_subjs) == 2 and sentence_split[sent_subjs[0]].lower() == sentence_split[sent_subjs[1]].lower():\n",
    "                del sentence_split[sent_subjs[1]] # remove the second occurence of the subject\n",
    "                corrupted_data.write(f'{i}\\t{\" \".join(sentence_split)}\\n')\n",
    "                continue\n",
    "\n",
    "            # case: vad som\n",
    "            # simply check for \"vad som\" - deprel label of som seems to be Unclassifiable grammatical function\n",
    "            if \"vad som\" in sentence:\n",
    "                som_indices = [i for i, x in enumerate(sentence_split) if x == \"som\"]\n",
    "                som_index = som_indices[0]\n",
    "                # check that \"som\" follows \"vad\" e.g in sentences like \"M√§nniskor som g√∂r vad som helst\"\n",
    "                if len(som_indices) > 1:\n",
    "                    for i in som_indices:\n",
    "                        if sentence_split[som_index-1].lower() == \"vad\":\n",
    "                            break\n",
    "                        som_index = i\n",
    "                del sentence_split[sentence_split.index(\"som\")]\n",
    "                corrupted_data.write(f'{i}\\t{\" \".join(sentence_split)}\\n')\n",
    "                continue\n",
    "\n",
    "            # case: drop prounoun subject\n",
    "            # SS(PN) MS .... -> MS ....\n",
    "            for sub in sent_subjs:\n",
    "                if 'PN' in tagged_dict[sub]['msd'] and (sub + 1) < len(tagged_dict) and 'VB' in tagged_dict[sub + 1]['msd']:\n",
    "                    del sentence_split[sub]\n",
    "                    # capitalise new first token\n",
    "                    if sub == 0:\n",
    "                        sentence_split[0] = sentence_split[0].capitalize()\n",
    "                    corrupted_data.write(f'{i}\\t{\" \".join(sentence_split)}\\n')\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-playback",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
